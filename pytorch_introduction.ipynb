{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and DataLoaders in PyTorch\n",
    "\n",
    "- To decouple data processing code from the model training code, PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset.\n",
    "- These allow you to use pre-loaded datasets as well as your own data.\n",
    "- Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "- PyTorch domain libraries provide a number of pre-loaded datasets (such as FashionMNIST) that subclass torch.utils.data.Dataset and implement functions specific to the particular data. They can be used to prototype and benchmark your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:06<00:00, 4.13MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 1.06MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.65MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 5.13MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create a custom dataset for your files\n",
    "\n",
    "- A custom Dataset class must implement three functions: init, len, and getitem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DataLoader is an iterable that abstracts this complexity for us in an easy API.\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through the DataLoader\n",
    "\n",
    "Once we load the dataset into the DataLoader, we can iterate through the dataset as needed. Each iteration returns a batch of train_features and train_labels (containing batch_size=64 features and labels respectively). If shuffle=True, after we iterate over all batches, the data is shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms\n",
    "- Transforms are used in PyTorch to perform manipulations on data to transform it into a form that is required for training machine learning algorithms.\n",
    "- All TorchVision datasets have two parameters — transform to modify the features and target_transform to modify the labels - that accept callables containing the transformation logic. The torchvision.transforms module offers several commonly-used transforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the code below, we use ToTensor() and Lambda() to transform the FashionMNIST features and corresponding labels. The FashionMNIST features are in PIL Image format, and the labels are integers. For training, we need the features as normalized tensors, and the labels as one-hot encoded tensors. These transformations are made using ToTensor and Lambda. ToTensor converts a PIL image or NumPy ndarray into a FloatTensor, and scales the image's pixel intensity values in the range [0., 1.]\n",
    "\n",
    "- Lambda transforms apply any user-defined lambda function. Here, we define a function to turn the integer into a one-hot encoded tensor. It first creates a zero tensor of size 10 (the number of labels in our dataset) and calls scatter_ which assigns a value=1 on the index as given by the label y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build up pytorch model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the NeuralNetwork Class to represent your neural network\n",
    "\n",
    "We define our neural network by subclassing nn.Module, and initialize the neural network layers in init. Every nn.Module subclass implements the operations on input data in the forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Sequential is an ordered container of modules. The data is passed through all the modules in the same order as defined.\n",
    "\n",
    "The nn.Flatten layer converts each 2D 28x28 image into a contiguous array of 784 pixel values ( the minibatch dimension (at dim=0) is maintained).\n",
    "\n",
    "The linear layer is a module that applies a linear transformation on the input using its stored weights and biases.\n",
    "\n",
    "nn.ReLU Layer: Non-linear activations create the complex mappings between the model’s inputs and outputs. They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena. Non-linear activations like ReLU, SoftMax, Sigmoid, TanH are applied between linear layers in a neural network to introduce non-linearity. Here we use nn.ReLU on the hidden1 layer.\n",
    "\n",
    "Model Parameters: \n",
    "- Layers inside the model have parameters — weights and biases. These parameters are optimized during the model training.\n",
    "\n",
    "- When you create a model by subclassing the nn.Module, Pytorch automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s parameters() or named_parameters() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing the input data through the model and get the predicted probabilities.\n",
    "\n",
    "After we create an instance of the model, we pass it input data. This initializes the model and executes the model’s forward operations, along with some background operations. You should not call model.forward() directly.\n",
    "\n",
    "Calling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of the raw predicted values for each class, and dim=1 corresponding to the individual values of each output.\n",
    "\n",
    "We get the prediction probabilities by passing it through an instance of the nn.Softmax module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([1])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "## we take a sample minibatch of 3 images of size 28x28 from the torch dataset of FashionMNIST and print the image size:\n",
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.0596,  0.0796, -0.2315,  0.1933, -0.3839,  0.6995, -0.2612,  0.1835,\n",
      "          0.1482,  0.0447, -0.5111, -0.0314, -0.9816, -0.5036, -0.5609,  0.0624,\n",
      "         -0.3071, -0.4158, -0.1008, -0.1150],\n",
      "        [-0.0598,  0.0239,  0.1319,  0.3473,  0.0788,  0.3962, -0.2413,  0.2170,\n",
      "          0.1315,  0.3514, -0.3562, -0.2962, -0.5974, -0.1135, -0.1503,  0.1772,\n",
      "         -0.2540, -0.4160,  0.1720, -0.0084],\n",
      "        [-0.1264, -0.0827,  0.1613,  0.0748, -0.3461,  0.7100, -0.1327, -0.0901,\n",
      "          0.0487,  0.3878, -0.4721, -0.4221, -0.9633, -0.2967, -0.2209, -0.2089,\n",
      "         -0.0381, -0.5185, -0.0670, -0.1023]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.0796, 0.0000, 0.1933, 0.0000, 0.6995, 0.0000, 0.1835, 0.1482,\n",
      "         0.0447, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0624, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0239, 0.1319, 0.3473, 0.0788, 0.3962, 0.0000, 0.2170, 0.1315,\n",
      "         0.3514, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1772, 0.0000, 0.0000,\n",
      "         0.1720, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1613, 0.0748, 0.0000, 0.7100, 0.0000, 0.0000, 0.0487,\n",
      "         0.3878, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0108,  0.0153, -0.0099,  ..., -0.0002, -0.0151, -0.0124],\n",
      "        [-0.0083,  0.0357,  0.0050,  ..., -0.0262, -0.0335, -0.0349]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([0.0056, 0.0106], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0164, -0.0037,  0.0236,  ...,  0.0298,  0.0306, -0.0302],\n",
      "        [-0.0183, -0.0032,  0.0421,  ...,  0.0350, -0.0330,  0.0137]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0372,  0.0340], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0131, -0.0342, -0.0118,  ..., -0.0186,  0.0400, -0.0189],\n",
      "        [-0.0140, -0.0183,  0.0043,  ..., -0.0101, -0.0158,  0.0137]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0050, -0.0027], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, neural networks use back propagation algorithm to adjust the model parameters based on the gradient of the loss function with respect to the given parameter.\n",
    "\n",
    "To compute those gradients, PyTorch has a built-in differentiation engine called torch.autograd. It supports automatic computation of gradient for any computational graph.\n",
    "\n",
    "Computational Graph\n",
    "\n",
    "A computational graph is a graph data structure in which the nodes represent mathematical operators except for one case, where we need to represent creation of a user-defined variable. Creation of a computational graph allows us to easily compute the gradients in a neural network regardless of the architecture type of the neural network.\n",
    "\n",
    "To demonstrate the computation of gradient, we will create a simplest, one-layer neural network, with input x, parameters w and b, and some loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this network, w and b are parameters, which we need to optimize. Thus, we need to be able to compute the gradients of loss function with respect to those variables. To do that, we set the requires_grad property of those tensor.\n",
    "\n",
    "A function that we apply to tensors to construct computational graph is an object of class Function. This object knows how to compute the function in the forward direction, and also how to compute its derivative during the backward propagation step. A reference to the backward propagation function is stored in grad_fn property of a tensor. grad_fn is a function \"handle\", giving access to the applicable gradient function. The gradient at the given point is a coefficient for adjusting weights during back-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x000001F87149DB40>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x000001F875E33F70>\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the derivatives of our loss function with respect to parameters, namely, we need ∂𝑙𝑜𝑠𝑠/∂𝑤 and ∂𝑙𝑜𝑠𝑠/∂𝑏 under some fixed values of x and y, to compute those derivatives, we call loss.backward(), and then retrieve the values from w.grad and b.grad:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can only obtain the grad properties for the leaf nodes of the computational graph, which have requires_grad property set to True. For all other nodes in our graph, gradients will not be available.\n",
    "We can only perform gradient calculations using backward once on a given graph, for performance reasons. If we need to do several backward calls on the same graph, we need to pass retain_graph=True to the backward call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can diasble gradient tracking:\n",
    "\n",
    "By default, all tensors with requires_grad=True are tracking their computational history and support gradient computation. However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do forward computations through the network.\n",
    "\n",
    "You might also want to disable gradient tracking to mark some parameters in your neural network as frozen parameters and to speed up computations when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0027, 0.2776, 0.0252],\n",
      "        [0.0027, 0.2776, 0.0252],\n",
      "        [0.0027, 0.2776, 0.0252],\n",
      "        [0.0027, 0.2776, 0.0252],\n",
      "        [0.0027, 0.2776, 0.0252]])\n",
      "tensor([0.0027, 0.2776, 0.0252])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can stop tracking computations by surrounding our computation code with torch.no_grad() block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to achieve the same result is to use the detach() method on the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch’s Autograd keeps a record of data (tensors) and all executed operations (along with the resulting new tensors) in a computational graph which is a directed acyclic graph (DAG) consisting of Function objects. In this DAG, leaves are the input tensors, roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule. In a forward pass, autograd does two things simultaneously:\n",
    "\n",
    "run the requested operation to compute a resulting tensor\n",
    "maintain the operation’s gradient function in the DAG.\n",
    "The backward pass starts when .backward() is called on the DAG root. autograd then:\n",
    "\n",
    "computes the gradients from each .grad_fn,\n",
    "accumulates them in the respective tensor’s .grad attribute\n",
    "using the chain rule, propagates all the way to the leaf tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Model Parameters\n",
    "\n",
    "After creating a model and having the data, we will train, validate and test our model by optimizing its parameters on our data.\n",
    "\n",
    "Training a model is an iterative process; in each iteration the model makes a guess about the output, calculates the error in its guess (loss), collects the derivatives of the error with respect to its parameters (as we saw in the previous section), and optimizes these parameters using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optimization Loop\n",
    "\n",
    "Once we set our hyperparameters, we can then train and optimize our model with an optimization loop. Each iteration of the optimization loop is called an epoch.\n",
    "\n",
    "Each epoch consists of two main parts:\n",
    "\n",
    "- The Train Loop — iterate over the training dataset and try to converge to optimal parameters.\n",
    "- The Validation/Test Loop — iterate over the test dataset to check if model performance is improving.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "When presented with some training data, our untrained network is likely not to give the correct answer. Loss function measures the degree of dissimilarity of obtained result to the target value, and it is the loss function that we want to minimize during training. To calculate the loss we make a prediction using the inputs of our given data sample and compare it against the true data label value.\n",
    "\n",
    "Common loss functions include nn.MSELoss (Mean Square Error) for regression tasks, and nn.NLLLoss (Negative Log Likelihood) for classification. nn.CrossEntropyLoss combines nn.LogSoftmax and nn.NLLLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Optimization is the process of adjusting model parameters to reduce model error in each training step. Optimization algorithms define how this process is performed. All optimization logic is encapsulated in the optimizer object. Here, we use the SGD optimizer. There are many different optimizers available in PyTorch such as ADAM and RMSProp, that work better for different kinds of models and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the training loop, optimization happens in three steps:\n",
    "\n",
    "- Call optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
    "- Backpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
    "- Once we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.305879  [   64/60000]\n",
      "loss: 2.301374  [ 6464/60000]\n",
      "loss: 2.287092  [12864/60000]\n",
      "loss: 2.282495  [19264/60000]\n",
      "loss: 2.260978  [25664/60000]\n",
      "loss: 2.239105  [32064/60000]\n",
      "loss: 2.236865  [38464/60000]\n",
      "loss: 2.209741  [44864/60000]\n",
      "loss: 2.217898  [51264/60000]\n",
      "loss: 2.173140  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 42.4%, Avg loss: 2.176937 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.190169  [   64/60000]\n",
      "loss: 2.169486  [ 6464/60000]\n",
      "loss: 2.149899  [12864/60000]\n",
      "loss: 2.140035  [19264/60000]\n",
      "loss: 2.074665  [25664/60000]\n",
      "loss: 2.079767  [32064/60000]\n",
      "loss: 2.075389  [38464/60000]\n",
      "loss: 2.010519  [44864/60000]\n",
      "loss: 2.024124  [51264/60000]\n",
      "loss: 1.966466  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 1.938069 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.942188  [   64/60000]\n",
      "loss: 1.911282  [ 6464/60000]\n",
      "loss: 1.852633  [12864/60000]\n",
      "loss: 1.784628  [19264/60000]\n",
      "loss: 1.745510  [25664/60000]\n",
      "loss: 1.737626  [32064/60000]\n",
      "loss: 1.773935  [38464/60000]\n",
      "loss: 1.661507  [44864/60000]\n",
      "loss: 1.643789  [51264/60000]\n",
      "loss: 1.571911  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 1.572025 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.608262  [   64/60000]\n",
      "loss: 1.472185  [ 6464/60000]\n",
      "loss: 1.486952  [12864/60000]\n",
      "loss: 1.401766  [19264/60000]\n",
      "loss: 1.401037  [25664/60000]\n",
      "loss: 1.397544  [32064/60000]\n",
      "loss: 1.314758  [38464/60000]\n",
      "loss: 1.379859  [44864/60000]\n",
      "loss: 1.353474  [51264/60000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     test_loop(test_dataloader, model, loss_fn)\n",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Set the model to training mode - important for batch normalization and dropout layers\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Unnecessary in this situation but added for best practices\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 6\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Compute prediction and loss\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fa028\\GitHub\\Time-Series\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\fa028\\GitHub\\Time-Series\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\fa028\\GitHub\\Time-Series\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\fa028\\GitHub\\Time-Series\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\fa028\\GitHub\\Time-Series\\venv\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\fa028\\GitHub\\Time-Series\\venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:129\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    127\u001b[0m     _log_api_usage_once(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_tensor(pic)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load the Model\n",
    "\n",
    "PyTorch models store the learned parameters in an internal state dictionary, called state_dict. We can use torch.save method to save these parameters to persistant storage (hard disk).\n",
    "\n",
    "Saving a model in this way will save the entire module using Python’s pickle module. A common PyTorch convention is to save models using either a .pt or .pth file extension.\n",
    "\n",
    "You must call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "# Model class must be defined somewhere\n",
    "model = torch.load(PATH)\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
